apiVersion: v1
kind: Namespace
metadata:
  name: odinfold
  labels:
    app: odinfold
    engine: foldforever
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: odinfold-config
  namespace: odinfold
  labels:
    app: odinfold
data:
  config.yaml: |
    odinfold:
      engine: "FoldForever"
      version: "1.0.0"
      model:
        name: "OdinFold"
        type: "protein-folding"
        quantization: "8bit"
      gpu:
        enabled: true
        memory_limit: "16Gi"
        compute_capability: "7.0"
      benchmark:
        tm_threshold: 0.66
        runtime_threshold: 5.5
        memory_threshold: 8.0
      azure:
        enabled: true
        auto_scale: true
        cost_optimization: true
---
apiVersion: v1
kind: Secret
metadata:
  name: odinfold-secrets
  namespace: odinfold
  labels:
    app: odinfold
type: Opaque
data:
  # Add your secrets here (base64 encoded)
  # huggingface_token: <base64-encoded-token>
  # azure_storage_key: <base64-encoded-key>
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: odinfold-api
  namespace: odinfold
  labels:
    app: odinfold
    component: api
    engine: foldforever
spec:
  replicas: 2
  selector:
    matchLabels:
      app: odinfold
      component: api
  template:
    metadata:
      labels:
        app: odinfold
        component: api
        engine: foldforever
    spec:
      serviceAccountName: odinfold-sa
      containers:
      - name: odinfold-api
        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
        ports:
        - containerPort: 8000
          name: http
        - containerPort: 8080
          name: metrics
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "8Gi"
            cpu: "2"
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4"
        env:
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: NVIDIA_VISIBLE_DEVICES
          value: "all"
        - name: NVIDIA_DRIVER_CAPABILITIES
          value: "compute,utility"
        - name: ODINFOLD_CONFIG
          value: "/etc/odinfold/config.yaml"
        - name: PYTHONPATH
          value: "/workspace/odinfold"
        volumeMounts:
        - name: config
          mountPath: /etc/odinfold
        - name: workspace
          mountPath: /workspace
        command: ["/bin/bash"]
        args:
        - -c
        - |
          echo "âš¡ Starting OdinFold API Server"
          
          # Install dependencies
          pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
          pip install fastapi uvicorn numpy scipy matplotlib seaborn pandas psutil
          pip install transformers accelerate bitsandbytes
          
          # Clone OdinFold
          git clone https://github.com/euticus/openfold.git /workspace/odinfold
          cd /workspace/odinfold
          pip install -e .
          
          # Start API server
          python -m uvicorn odinfold.api.server:app --host 0.0.0.0 --port 8000
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: config
        configMap:
          name: odinfold-config
      - name: workspace
        emptyDir:
          sizeLimit: 50Gi
      nodeSelector:
        accelerator: nvidia-tesla-a100
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: odinfold-api-service
  namespace: odinfold
  labels:
    app: odinfold
    component: api
spec:
  selector:
    app: odinfold
    component: api
  ports:
  - name: http
    port: 80
    targetPort: 8000
    protocol: TCP
  - name: metrics
    port: 8080
    targetPort: 8080
    protocol: TCP
  type: LoadBalancer
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: odinfold-benchmark
  namespace: odinfold
  labels:
    app: odinfold
    component: benchmark
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM UTC
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: odinfold
            component: benchmark
        spec:
          restartPolicy: Never
          containers:
          - name: odinfold-benchmark
            image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
            resources:
              requests:
                nvidia.com/gpu: 1
                memory: "16Gi"
                cpu: "4"
              limits:
                nvidia.com/gpu: 1
                memory: "32Gi"
                cpu: "8"
            env:
            - name: CUDA_VISIBLE_DEVICES
              value: "0"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
            - name: NVIDIA_DRIVER_CAPABILITIES
              value: "compute,utility"
            command: ["/bin/bash"]
            args:
            - -c
            - |
              echo "âš¡ Starting OdinFold Daily Benchmark"
              
              # Install dependencies
              pip install torch==2.0.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
              pip install numpy scipy matplotlib seaborn pandas psutil
              
              # Clone OdinFold
              git clone https://github.com/euticus/openfold.git /workspace/odinfold
              cd /workspace/odinfold
              pip install -e .
              
              # Run benchmark
              python scripts/evaluation/gpu_ci_benchmark.py \
                --config production \
                --tm-threshold 0.66 \
                --runtime-threshold 5.5 \
                --memory-threshold 8.0 \
                --output-dir /tmp/benchmark_results \
                --casp-benchmark \
                --performance-benchmark
              
              # Upload results (implement your storage solution)
              echo "ðŸ“Š Benchmark completed successfully!"
              cat /tmp/benchmark_results/ci_report.md
          nodeSelector:
            accelerator: nvidia-tesla-a100
          tolerations:
          - key: nvidia.com/gpu
            operator: Exists
            effect: NoSchedule
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: odinfold-api-hpa
  namespace: odinfold
  labels:
    app: odinfold
    component: api
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: odinfold-api
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 30
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: odinfold-sa
  namespace: odinfold
  labels:
    app: odinfold
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: odinfold-role
  labels:
    app: odinfold
rules:
- apiGroups: [""]
  resources: ["pods", "services", "configmaps", "secrets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "replicasets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: odinfold-binding
  labels:
    app: odinfold
subjects:
- kind: ServiceAccount
  name: odinfold-sa
  namespace: odinfold
roleRef:
  kind: ClusterRole
  name: odinfold-role
  apiGroup: rbac.authorization.k8s.io
