# ðŸš€ OdinFold Production Benchmark Guide

**Deploy to GPU servers and call from your local machine**

## ðŸŽ¯ Quick Start (3 Steps)

### 1ï¸âƒ£ **Deploy to GPU Servers**

On each GPU server, run:

```bash
# Make deployment script executable
chmod +x deploy.sh

# Deploy with dependency installation and GPU check
./deploy.sh --install-deps --gpu-check --port 8000
```

**Expected output:**
```
ðŸš€ OdinFold Production Benchmark Deployment
==========================================
ðŸ“‹ Configuration:
   Host: 0.0.0.0
   Port: 8000
   Conda env: openfold
ðŸ”§ Activating conda environment: openfold
ðŸ” Checking GPU availability...
NVIDIA A100-SXM4-80GB, 81920, 1024
PyTorch CUDA available: True
GPU: NVIDIA A100-SXM4-80GB
ðŸ“¦ Installing dependencies...
âœ… Dependencies installed
âœ… All required files found
ðŸŒ Starting OdinFold Benchmark Server...
   URL: http://0.0.0.0:8000
   Health check: http://0.0.0.0:8000/health
   API docs: http://0.0.0.0:8000/docs
ðŸ”¥ Server starting... (Ctrl+C to stop)
```

### 2ï¸âƒ£ **Test Server Health**

```bash
# Test from local machine
curl http://YOUR_GPU_SERVER:8000/health
```

**Expected response:**
```json
{
  "status": "healthy",
  "timestamp": 1704067200.0,
  "gpu_available": true,
  "gpu_name": "NVIDIA A100-SXM4-80GB",
  "gpu_memory_total": 85899345920,
  "gpu_memory_allocated": 0
}
```

### 3ï¸âƒ£ **Run Benchmark from Local Machine**

```bash
# Run distributed benchmark across multiple GPU servers
python benchmark_client.py \
  --servers http://gpu1:8000 http://gpu2:8000 http://gpu3:8000 \
  --sequences "MKWVTFISLLFLFSSAYS" "MKTVRQERLKSIVRILERSKEPVSGAQLAEELSVSRQVIVQDIAYLRSLGYNIVATPRGYVLAGG" \
  --models odinfold \
  --output production_results.json
```

## ðŸ“Š **Expected Benchmark Output**

```
ðŸš€ Starting distributed benchmark on 3 servers
âœ… Server http://gpu1:8000: NVIDIA A100-SXM4-80GB
âœ… Server http://gpu2:8000: NVIDIA A100-SXM4-80GB  
âœ… Server http://gpu3:8000: NVIDIA RTX 4090
ðŸ“‹ Starting job distributed_job_1704067200_0 on http://gpu1:8000 with 1 sequences
ðŸ“‹ Starting job distributed_job_1704067200_1 on http://gpu2:8000 with 1 sequences
â³ Waiting for job distributed_job_1704067200_0 on http://gpu1:8000
ðŸ“Š Job distributed_job_1704067200_0: running (50.0%)
ðŸ“Š Job distributed_job_1704067200_0: completed (100.0%)
âœ… Job distributed_job_1704067200_0 completed successfully!
ðŸŽ¯ Distributed benchmark completed!

ðŸŽ¯ BENCHMARK RESULTS SUMMARY
========================================
ðŸ“Š Total sequences: 2
ðŸ–¥ï¸  Servers used: 3
âœ… Success rate: 100.0%
â±ï¸  Average runtime: 1.23s
ðŸ’¾ Average GPU memory: 2847.5MB
ðŸš€ Throughput: 1.63 seq/s
ðŸ’¾ Results saved to: production_results.json
```

## ðŸ”§ **Advanced Configuration**

### Custom Benchmark Config

Create `benchmark_config.json`:

```json
{
  "models": {
    "odinfold": {
      "enabled": true,
      "weights_path": "openfold/resources/openfold_params/openfold_model_1_ptm.pt",
      "config_preset": "model_1_ptm"
    }
  },
  "datasets": {
    "casp14": {
      "fasta_dir": "casp14_data/fasta",
      "pdb_dir": "casp14_data/pdb",
      "enabled": true
    }
  },
  "hardware": {
    "device": "cuda",
    "mixed_precision": true,
    "max_memory_gb": 40
  }
}
```

### Multiple Models

```bash
python benchmark_client.py \
  --servers http://gpu1:8000 http://gpu2:8000 \
  --models odinfold openfold esmfold \
  --sequences "MKWVTFISLLFLFSSAYS" \
  --output multi_model_results.json
```

### CASP14 Dataset Benchmark

```bash
# Benchmark full CASP14 dataset
python benchmark_client.py \
  --servers http://gpu1:8000 http://gpu2:8000 http://gpu3:8000 \
  --sequences $(cat casp14_sequences.txt) \
  --models odinfold \
  --output casp14_results.json
```

## ðŸ› **Troubleshooting**

### Server Won't Start

```bash
# Check conda environment
conda env list

# Check GPU availability
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Check port availability
netstat -tulpn | grep :8000
```

### Client Connection Issues

```bash
# Test server connectivity
curl http://YOUR_GPU_SERVER:8000/health

# Check firewall
sudo ufw status
```

### Memory Issues

```bash
# Monitor GPU memory during benchmark
watch -n 1 nvidia-smi

# Reduce batch size in config
# Edit benchmark_config.json -> hardware.max_memory_gb
```

## ðŸ“ˆ **Performance Optimization**

### For A100 (80GB)
- Use `mixed_precision: true`
- Set `max_memory_gb: 70`
- Enable FlashAttention if available

### For RTX 4090 (24GB)
- Use `mixed_precision: true`
- Set `max_memory_gb: 20`
- Reduce sequence length if needed

### For Multiple GPUs
- Deploy one server per GPU
- Use different ports: 8000, 8001, 8002, etc.
- Load balance sequences across servers

## ðŸŽ¯ **Production Checklist**

- [ ] GPU servers accessible from local machine
- [ ] Conda environment `openfold` exists on all servers
- [ ] OpenFold model weights downloaded
- [ ] Firewall allows access to benchmark ports
- [ ] Sufficient GPU memory for target sequences
- [ ] Network bandwidth adequate for result transfer

## ðŸ“ž **API Reference**

### Health Check
```bash
GET /health
```

### Start Benchmark
```bash
POST /benchmark
{
  "sequences": ["MKWVTFISLLFLFSSAYS"],
  "models": ["odinfold"],
  "job_id": "my_job_123"
}
```

### Get Job Status
```bash
GET /benchmark/{job_id}
```

### Get Results
```bash
GET /benchmark/{job_id}/results
```

---

**ðŸš€ You're now ready for production OdinFold benchmarking!**
