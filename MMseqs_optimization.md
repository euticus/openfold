MMseqs Optimization

Below is a single, self-contained markdown playbook that spells out T-1 ‚Üí T-16 in excruciating (some might say lovingly pedantic) detail. Feel free to paste it straight into Cursor, Codex, GitHub Issues, Notion‚Äâ‚Äî‚Äâwherever you track work items.

# FoldForever ‚Ä¢ OpenFold++ Integration Roadmap  
## Tasks T-1 ‚Üí T-16 (super-verbose edition)

---

> **Legend**  
> ‚Ä¢ **Effort**‚ÄÉ‚âà wall-clock engineering time for one competent dev who already knows the code-base and has a working CUDA setup.  
> ‚Ä¢ **Ownership**‚ÄÉsuggested persona (swap as you wish).  
> ‚Ä¢ **Artifacts**‚ÄÉwhat must exist in the repo *and* pass CI for the task to count as ‚ÄúDone‚Äù.  
> ‚Ä¢ **Acceptance Test**‚ÄÉthe specific command(s) we‚Äôll run in CI/vscode to assert it works.  
> ‚Ä¢ **Gotchas**‚ÄÉcommon pitfalls + how to dodge them.  
> ‚Ä¢ **Reading List**‚ÄÉlinks / papers / sections of your own code worth skimming **before** writing a single line.

---

## T-1 ‚Ä¢ *Distil ESM ‚Üí ESM-Lite (INT4)*

| Field | Value |
|-------|-------|
| **Effort** | 1.5 days |
| **Ownership** | `@ml-engineer‚Äêsequence` |
| **Artifacts** | `models/esm_lite_int4.pt` (‚â§ 40 MB) + inference script `inference/esm_lite.py` |
| **Acceptance Test** | `python inference/esm_lite.py --seq "ACDEFGHIKLMNPQRSTVWY" --benchmark` must print **‚Äúlatency ‚â§ 30 ms/100aa‚Äù** on an H100 |
| **Gotchas** | *Don‚Äôt* quantise LayerNorm weights until after SmoothQuant scaling; it nukes stability. |
| **Reading List** | ‚Ä¢ ESM-2 paper ¬ß4.2 ‚ÄÉ‚Ä¢ SmoothQuant README ‚ÄÉ‚Ä¢ `facebookresearch/esm` repo `generate.py` |

**Verbose Directions**

1. **Fork** the official `facebookresearch/esm` repo into `github.com/yourOrg/esm-lite`.  
2. **Create** a *distillation script* (`scripts/distil_esm.py`) that:  
   1. Loads the 650 M ESM-2 FP16 checkpoint.  
   2. Keeps every 4th transformer block (so 48 ‚Üí 12).  
   3. Freezes token + positional embeddings; trains only retained blocks for 2 epochs on UniRef50.  
   4. Adds a *64-D projection head* (will be replaced in T-2).  
3. **Quantise** with ü§ó `bitsandbytes` *NF4* (weights) + SmoothQuant (activations).  
4. **Export** final weights to `models/esm_lite_int4.pt`.  
5. **Write** `inference/esm_lite.py` that exposes `embed(sequence:str)->torch.Tensor`.  
6. **Benchmark** on a single H100; ensure latency budget is met; tweak batch-size if needed.  
7. Push + open a PR tagged `feat/esm-lite-int4`.

---

## T-2 ‚Ä¢ *Token-Bottleneck ‚ÄúCHEAP-S‚Äù Projection Head*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@ml-engineer‚Äêsequence` |
| **Artifacts** | `models/esm_lite_int4_cheaps.pt` + `layers/cheap_head.py` |
| **Acceptance Test** | `pytest tests/test_cheap_head.py::test_shape_and_latency` must pass |

**Verbose Directions**

1. Implement **`CheapHead(nn.Module)`**: two linear layers 64 ‚Üí 256 ‚Üí 64 with **ReLU** in between and residual skip.  
2. Drop-in replace the projection head added in T-1.  
3. Retrain **only** the head for 1 epoch with a cosine LR schedule.  
4. Update unit test to assert output shape `(len(seq), 64)` and < 3 ms overhead.  
5. Re-export weights ‚Üí `esm_lite_int4_cheaps.pt`.

---

## T-3 ‚Ä¢ *Shingle Encoder + FAISS HNSW-GPU Index*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@backend-engineer-search` |
| **Artifacts** | `data/motif_index.faiss` (‚âà 30 GB) ‚Ä¢ CLI `tools/build_index.py` |
| **Acceptance Test** | `python tools/build_index.py --dry-run` must finish < 10 s and report ‚Äúindex loaded‚Äù |
| **Gotchas** | Use `faiss.IndexHNSWFlat` *NOT* `IVFPQ` (too slow to add > 500 M items). |
| **Reading List** | Faiss HNSW tutorial ‚ÄÉ‚Ä¢ FoldSeek code `generateDB.c++` |

**Verbose Directions**

1. **Split** every protein in AFDB into overlapping 10-token shingles (`stride=1`).  
2. **Map** each shingle to a 64-D vector via `CheapHead(embed(...))`.  
3. **Add** to `IndexHNSWFlat`, `efConstruction=128`, `M=32`.  
4. **Persist** index to `data/motif_index.faiss` and commit via Git LFS (pointer only).  
5. Write retrieval API `search/motif_retriever.py`.

---

## T-4 ‚Ä¢ *Triton Kernel: 2-Simplicial Attention*

| Field | Value |
|-------|-------|
| **Effort** | 2 days |
| **Ownership** | `@cuda-sorcerer` |
| **Artifacts** | `kernels/simplicial_attn.cu` + PyBind wrapper `bindings/simplicial_attn.cpp` |
| **Acceptance Test** | `pytest tests/test_simplicial_attn.py::test_max_error` must show < 1e-3 diff vs. Torch ref |

**Verbose Directions**

1. Sketch math: three inputs `(A_i, A_j, A_k)` ‚Üí tri-linear dot ‚Üí score ‚Üí softmax.  
2. Tile 64√ó128 in shared memory; schedule outer loops so warp 0 writes softmax denom.  
3. Use Hopper **DPX** instructions via `mma.sync.aligned.m16n8k8.row.col`.  
4. Export function `void run_simplicial(float* qkv, float* out, int L, int d, cudaStream_t)`.  
5. Bind to Python. Provide fall-back path (Torch einsum) if `!has_cuda_sm90`.

---

## T-5 ‚Ä¢ *Graph Compositor Training Script*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@ml-engineer-geometry` |
| **Artifacts** | `models/compositor_int8.pt` + script `train/train_compositor.py` |
| **Acceptance Test** | `python train/train_compositor.py --smoke-test` must overfit 10 samples to TM-score > 0.9 |

**Verbose Directions**

* Build PyTorch-Lightning module `GraphCompositor`.  
* Node features = motif embeddings, edge features = clash/offset scalars.  
* Loss = `1 ‚Äì average_TM_score(coarse, native)`.  
* INT8 quant aware training; export weights + calibration json.

---

## T-6 ‚Ä¢ *TinyDiFF UNet Definition & Scheduler*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@ml-engineer-diffusion` |
| **Artifacts** | `models/tinydiff_unet.pt` ‚Ä¢ `models/tinydiff_scheduler.pkl` ‚Ä¢ code `models/tinydiff.py` |
| **Acceptance Test** | `python models/tinydiff.py --self-test` must predict coords for 50aa toy seq < 50 ms |

**Verbose Directions**

1. Implement UNet with downsample (Conv + SiLU) ‚Üí bottleneck (SE(3) equiv. layers) ‚Üí upsample.  
2. Time-embed via sinusoidal+linear.  
3. **Scheduler** = cosine noise schedule, 8 steps, no classifier-free guidance.  
4. Provide `predict(sequence:str)->np.ndarray`.

---

## T-7 ‚Ä¢ *Distillation Pipeline Notebook*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@ml-engineer-diffusion` |
| **Artifacts** | `notebooks/distil_pipeline.ipynb` |
| **Acceptance Test** | CI executes notebook non-interactively via `papermill` w/o error |

**Verbose Directions**

Notebook steps: fetch AFDB json ‚Üí generate `(coarse, fine)` pairs ‚Üí train TinyDiFF with L2D trick ‚Üí export to `models/`. Include graphs of loss curves + sample visualisations.

---

## T-8 ‚Ä¢ *End-to-End Driver (`predict.py`)*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@platform-engineer` |
| **Artifacts** | `bin/predict.py` (CLI) + `grpc/predict.proto` |
| **Acceptance Test** | `bin/predict.py tests/data/4insA.fasta --out out.pdb` produces file and logs total latency < 120 ms |

**Verbose Directions**

1. `embed ‚Üí shingle search ‚Üí compositor ‚Üí tinydiff` pipeline.  
2. CLI flags to toggle **MD polish** and choose output `pdb|mmCIF|json`.  
3. Optional gRPC mode (`--serve`).  

---

## T-9 ‚Ä¢ *CUDA 12.4 Docker Image*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@devops-guru` |
| **Artifacts** | `Dockerfile` + GHCR image `foldforever/core:latest` |
| **Acceptance Test** | `docker run --gpus all foldforever/core:latest bin/predict.py ‚Ä¶` passes |

**Verbose Directions**

‚Ä¢ Base on `nvidia/cuda:12.4.1-cudnn-runtime-ubuntu22.04`  
‚Ä¢ Install FlashAttention-2, Triton nightly, Faiss-gpu, PyTorch w/ SM90.  
‚Ä¢ Copy compiled kernels + models via multistage build.  
‚Ä¢ Enable `ENTRYPOINT ["bin/predict.py"]` by default.

---

## T-10 ‚Ä¢ *Terraform + Helm Charts*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@devops-guru` |
| **Artifacts** | `infra/terraform/` ‚Ä¢ `infra/charts/foldforever/` |
| **Acceptance Test** | `make deploy-k8s` spins one H100 node, deploys service, health-endpoint returns 200 |

**Verbose Directions**

1. **Terraform** provisions GPU node-pool (e.g., Azure NDP H100-80GB).  
2. **Helm chart** mounts model volume via PVC, sets `GPU=1`, autoscaler params.  
3. Service type `ClusterIP` + Istio virtual service for ingress.

---

## T-11 ‚Ä¢ *MotifAdapter (`motif_adapter.py`)*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@ml-engineer-bridge` |
| **Artifacts** | `openfoldpp/adapters/motif_adapter.py` |
| **Acceptance Test** | Unit test validates tensor shapes `(L, 256)` vs. vanilla OpenFold features |

**Verbose Directions**

* Accepts list of motif hits `(coords, tm, offset, len)`.  
* Generates pair-feature tensor with channels: TM-score, œÜ, œà, distance, clash flag, etc.  
* Aligns to OpenFold expected dtype/ordering (`float32`, features last).

---

## T-12 ‚Ä¢ *C API + pybind11 Wrapper for Triton Compositor*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@cuda-sorcerer` |
| **Artifacts** | `bindings/simplicial_attn.so` |
| **Acceptance Test** | `python -c "import simplicial_attn as sa; sa.version()"` prints commit hash |

**Verbose Directions**

1. Expose single function `run_compositor(float* inEmb, float* outPair, int L)`.  
2. Build with `set(CMAKE_POSITION_INDEPENDENT_CODE ON)` so openfold++ can link.  
3. Provide `setup.py` for editable install.

---

## T-13 ‚Ä¢ *CoordAdapter (`coord_adapter.cpp`)*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@ml-engineer-bridge` |
| **Artifacts** | `openfoldpp/adapters/coord_adapter.cpp` |
| **Acceptance Test** | Running `predict_openfoldpp.py` produces `.pkl` consumable by existing visualization scripts |

**Verbose Directions**

* Convert TinyDiFF numpy array `[L, 37, 3]` ‚Üí OpenFold‚Äôs `atom_positions`, fill `alt_atom_positions` w/ zeros.  
* Compute pLDDT proxy from diffusion variance; clamp 0-100.  
* Serialize with OpenFold‚Äôs `protein.Protein` dataclass ‚Üí pickle.

---

## T-14 ‚Ä¢ *CLI Driver `predict_openfoldpp.py`*

| Field | Value |
|-------|-------|
| **Effort** | 0.5 day |
| **Ownership** | `@platform-engineer` |
| **Artifacts** | `bin/predict_openfoldpp.py` |
| **Acceptance Test** | For test FASTA, script emits identical `.pkl` schema as vanilla OpenFold reference run |

**Verbose Directions**

* Glue together tasks T-1 ‚Üí T-13.  
* `--skip-diffusion` flag for ablation.  
* Saves both PDB and Pickle side-by-side.

---

## T-15 ‚Ä¢ *Unit / Integration Tests*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@qa-engineer` |
| **Artifacts** | `tests/` suite + CI pipeline step |
| **Acceptance Test** | `pytest -q` must show 100 % pass; coverage ‚â• 85 % |

**Verbose Directions**

* Smoke tests for each adapter.  
* Numerical equivalence tests against gold-standard structures.  
* CI matrix: CPU-only (mocks) + GPU (real).  
* GitHub Action caches FAISS index to avoid 30 GB download each run.

---

## T-16 ‚Ä¢ *Streaming RPC Service*

| Field | Value |
|-------|-------|
| **Effort** | 1 day |
| **Ownership** | `@backend-engineer` |
| **Artifacts** | `grpc/foldforever.proto` ‚Ä¢ server `services/predict_server.py` |
| **Acceptance Test** | `grpcurl -d @ -plaintext localhost:50051 foldforever.Predict/Run < tests/data/request.json` returns JSON with `plddt` field |

**Verbose Directions**

* Re-use existing OpenFold gRPC schema for seamless UI swap.  
* Implement bidirectional streaming so client can receive progress updates (percent tokens, motif phase, diff phase).  
* Add simple **prometheus** metrics (`fold_time_ms`, `gpu_mem_bytes`) and healthcheck endpoint `/healthz`.

---

### üöÄ How to Execute the Roadmap

1. **Sprint-plan**: Put T-1 ‚Üí T-3 + T-11 first (data + key adapter).  
2. **Parallel tracks**:  
   * CUDA guru handles T-4 + T-12.  
   * Diffusion lead tackles T-6 + T-7.  
3. **Milestone 1** (*Day 5*): `predict_openfoldpp.py` runs end-to-end on 100aa sample in < 200 ms.  
4. **Milestone 2** (*Day 10*): CI green on T-15; cost dashboard (¬ß p-token) shows ‚â§ \$0.0003/100aa.  
5. **Milestone 3** (*Day 12*): Streaming gRPC deployed via Helm ‚Üí web UI switch-over is one ENV flag.

---

*Copy-paste above into your repo‚Äôs `docs/roadmap.md` and you‚Äôre good to sprint!*  
Questions, edge-cases, or if you want code snippets for any specific subtask‚Äâ‚Äî‚Äâjust holler.
::contentReference[oaicite:0]{index=0}
